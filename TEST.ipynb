{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.metadata\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import time\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, TypeVar, Union\n",
    "import math\n",
    "import aeon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import aeon\n",
    "from aeon.datasets import load_from_tsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 500, 2) (500,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = aeon.datasets.load_from_tsfile(DATA_PATH + \"Blink_TRAIN.ts\")\n",
    "test_x, test_y = aeon.datasets.load_from_tsfile(DATA_PATH + \"Blink_TEST.ts\")\n",
    "\n",
    "train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "test_x, test_y = np.array(test_x), np.array(test_y)\n",
    "\n",
    "#reshape from (sample, feat_dim, seq_length) to (seq_length, sample, feat_dim)\n",
    "train_x, test_x = np.transpose(train_x, (2, 0, 1)), np.transpose(test_x, (2, 0, 1))\n",
    "\n",
    "# Separate x dimensions into 2 modalities\n",
    "m1_train_x = train_x[:, :, :2]\n",
    "m2_train_x = train_x[:, :, 2:]\n",
    "m1_train_y, m2_train_y = train_y, train_y\n",
    "\n",
    "#preserve labels\n",
    "m1_test_x = test_x[:, :2, :]\n",
    "m2_test_x = test_x[:, 2:, :]\n",
    "m1_test_y, m2_test_y = test_y, test_y\n",
    "\n",
    "print(m1_train_x.shape, m1_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Implemented from \"Language Modeling with nn.Transformer and TorchText\" \n",
    "\n",
    "    To inject positional information into the embeddings, we use add a embedding based on the mapping of sin/cosine to our original embedding. \n",
    "    REMARKS: do we need to add this if our representations already host positional information?\n",
    "\n",
    "    Args: \n",
    "        d_model: dimension of the embeddings, where embedding is shape [n_sample, seq_length, embedding_dim (d_model)]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(seq_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Optional[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[n_sample, seq_length, embedding_dim]``\n",
    "        \"\"\"\n",
    "        print(self.pe.shape)\n",
    "        print(x.shape)\n",
    "\n",
    "        #this transformation is for [n_sample, seq_length, embedding_dim]\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "        #this transformation is for [seq_length n_sample, embedding_dim]\n",
    "        #x = x + self.pe[:x.size(0)]\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_ = PositionalEncoding(d_model = 2, seq_len = 510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "hell0 = pe_(torch.Tensor(m1_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_attn_block(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Single Block for Cross Attention\n",
    "\n",
    "    Args: \n",
    "        m1: first modality\n",
    "        m2: second modality\n",
    "\n",
    "    Shapes: \n",
    "        m1: (seq_length, N_samples, N_features)\n",
    "        m2: (seq_length, N_samples, N_features)\n",
    "\n",
    "    Returns: \n",
    "        embedding of m1 depending on attending on certain elements of m2, multihead_attn(k_m1, v_m1, q_m2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 heads: int, \n",
    "                 dropout: float, \n",
    "                 seq_length: int):\n",
    "\n",
    "        super(cross_attn_block, self).__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(dim, dropout, seq_length)\n",
    "\n",
    "        #learnable\n",
    "        self._to_key = torch.nn.Linear(dim, dim)\n",
    "        self._to_query = torch.nn.Linear(dim, dim)\n",
    "        self._to_value = torch.nn.Linear(dim, dim)\n",
    "\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim = dim, num_heads = heads, dropout = dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                m1: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                m2: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \n",
    "        m1_x = self.positional_encoding(m1)\n",
    "        m2_x = self.positional_encoding(m2)\n",
    "        print(\"passed encoding\")\n",
    "\n",
    "        m1_k = self._to_key(m1_x)\n",
    "        m1_v = self._to_query(m1_x)\n",
    "        m2_q = self._to_value(m2_x)\n",
    "        print(\"passed kqv\")\n",
    "\n",
    "        #crossing\n",
    "        cross_x, attn_weights = self.attn(m1_k, m1_v, m2_q)\n",
    "        print(\"passed attn:\", cross_x.shape)\n",
    "\n",
    "        return cross_x\n",
    "\n",
    "\n",
    "class position_wise_ffn(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Position-wise feed-forward network with a RELU activation - essentially contracts output, and squeezes it back to the same space\n",
    "\n",
    "    ARGS:\n",
    "        dim: dimension of the embeddings\n",
    "        hidden_dim: dimension of the inflated hidden layer in feed-forward network\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 hidden_dim: int, \n",
    "                 dropout: float = 0.0):\n",
    "        super(position_wise_ffn, self).__init__()\n",
    "\n",
    "        self.ffn_1 = torch.nn.Linear(dim, hidden_dim)\n",
    "        self.ffn_2 = torch.nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.ffn_1(x).relu()\n",
    "        x = self.ffn_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class cross_attn_channel(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Model for Cross Attention, architecture implementation taken from encoder layer of \"Attention is all you need\"\n",
    "    Includes multi-head attn with crossing --> add + norm --> positionwise ffn --> add + norm --> output (based on paper)\n",
    "\n",
    "    ARGS:\n",
    "        dim_m1: time series modality 1\n",
    "        dim_m2: time series modality 2\n",
    "\n",
    "    Shapes:\n",
    "        assuming seq_length is same for both\n",
    "        m1: (seq_length, N_samples, N_features)\n",
    "        m2: (seq_length, N_samples, N_features)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim_m1: int, \n",
    "                 dim_m2: int, \n",
    "                 outdim_m1: int, \n",
    "                 outdim_m2: int,\n",
    "                 heads: Optional[int], \n",
    "                 seq_len: int, \n",
    "                 dropout: float = 0.0):\n",
    "        super(cross_attn_channel, self).__init__()\n",
    "\n",
    "        self.m1_cross_m2 = cross_attn_block(dim = dim_m1, heads = heads, dropout = dropout, seq_length = seq_len)\n",
    "        self.m2_cross_m1 = cross_attn_block(dim = dim_m2, heads = heads, dropout = dropout, seq_length = seq_len)\n",
    "\n",
    "        self.norm_m1 = torch.nn.LayerNorm(dim_m1)\n",
    "        self.norm_m2 = torch.nn.LayerNorm(dim_m2)\n",
    "\n",
    "        self.m1_pffn = position_wise_ffn(dim_m1, 512)\n",
    "        self.m2_pffn = position_wise_ffn(dim_m2, 512)\n",
    "\n",
    "        self.norm_pffn_m1 = torch.nn.LayerNorm(dim_m1)\n",
    "        self.norm_pffn_m2 = torch.nn.LayerNorm(dim_m2)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                m1: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                m2: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "\n",
    "        m1_x = self.m1_cross_m2(m1, m2)\n",
    "        m2_x = self.m2_cross_m1(m2, m1)\n",
    "\n",
    "        m1_x = self.norm_m1(m1 + self.dropout(m1_x))\n",
    "        m2_x  = self.norm_m2(m2 + self.dropout(m2_x))\n",
    "\n",
    "        m1_ffn = self.m1_pffn(m1_x)\n",
    "        m2_ffn = self.m2_pffn(m2_x)\n",
    "\n",
    "        m1_x = self.norm_pffn_m1(m1_x + self.dropout(m1_ffn))\n",
    "        m2_x = self.norm_pffn_m2(m2_x + self.dropout(m2_ffn))\n",
    "\n",
    "        return m1_x, m2_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 500, 2) (510, 500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(m2_train_x.shape, m1_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_channel_ = cross_attn_channel(dim_m1 = m1_train_x.shape[-1], dim_m2 = m2_train_x.shape[-1], outdim_m1=16, outdim_m2=16, heads = 2, seq_len =  m2_train_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "m1, m2 = cross_attn_channel_(torch.Tensor(m1_train_x), torch.Tensor(m2_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([510, 500, 2])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_train_x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iadam/miniconda3/envs/vtransformer_torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=2)\n",
    "transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "\n",
    "classifier = torch.nn.Linear(512, 2)\n",
    "out = classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 2])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMCA_simul",
   "language": "python",
   "name": "mmca_simul"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
