{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.metadata\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import time\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, TypeVar, Union\n",
    "import math\n",
    "import aeon\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import aeon\n",
    "from aeon.datasets import load_from_tsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 500, 2) (500,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = aeon.datasets.load_from_tsfile(DATA_PATH + \"Blink_TRAIN.ts\")\n",
    "test_x, test_y = aeon.datasets.load_from_tsfile(DATA_PATH + \"Blink_TEST.ts\")\n",
    "\n",
    "train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "test_x, test_y = np.array(test_x), np.array(test_y)\n",
    "\n",
    "#reshape from (sample, feat_dim, seq_length) to (seq_length, sample, feat_dim)\n",
    "train_x, test_x = np.transpose(train_x, (2, 0, 1)), np.transpose(test_x, (2, 0, 1))\n",
    "\n",
    "# Separate x dimensions into 2 modalities\n",
    "m1_train_x = train_x[:, :, :2]\n",
    "m2_train_x = train_x[:, :, 2:]\n",
    "m1_train_y, m2_train_y = train_y, train_y\n",
    "\n",
    "#preserve labels\n",
    "m1_test_x = test_x[:, :, :2]\n",
    "m2_test_x = test_x[:, :, 2:]\n",
    "m1_test_y, m2_test_y = test_y, test_y\n",
    "\n",
    "print(m1_train_x.shape, m1_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Implemented from \"Language Modeling with nn.Transformer and TorchText\" \n",
    "\n",
    "    To inject positional information into the embeddings, we use add a embedding based on the mapping of sin/cosine to our original embedding. \n",
    "    REMARKS: do we need to add this if our representations already host positional information?\n",
    "\n",
    "    Args: \n",
    "        d_model: dimension of the embeddings, where embedding is shape [n_sample, seq_length, embedding_dim (d_model)]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(seq_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Optional[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[n_sample, seq_length, embedding_dim]``\n",
    "        \"\"\"\n",
    "        print(self.pe.shape)\n",
    "        print(x.shape)\n",
    "\n",
    "        #this transformation is for [n_sample, seq_length, embedding_dim]\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "        #this transformation is for [seq_length n_sample, embedding_dim]\n",
    "        #x = x + self.pe[:x.size(0)]\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_ = PositionalEncoding(d_model = 2, seq_len = 510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "hell0 = pe_(torch.Tensor(m1_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_attn_block(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Single Block for Cross Attention\n",
    "\n",
    "    Args: \n",
    "        m1: first modality\n",
    "        m2: second modality\n",
    "\n",
    "    Shapes: \n",
    "        m1: (seq_length, N_samples, N_features)\n",
    "        m2: (seq_length, N_samples, N_features)\n",
    "\n",
    "    Returns: \n",
    "        embedding of m1 depending on attending on certain elements of m2, multihead_attn(k_m1, v_m1, q_m2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 heads: int, \n",
    "                 dropout: float, \n",
    "                 seq_length: int):\n",
    "\n",
    "        super(cross_attn_block, self).__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(dim, dropout, seq_length)\n",
    "\n",
    "        #learnable\n",
    "        self._to_key = torch.nn.Linear(dim, dim)\n",
    "        self._to_query = torch.nn.Linear(dim, dim)\n",
    "        self._to_value = torch.nn.Linear(dim, dim)\n",
    "\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim = dim, num_heads = heads, dropout = dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                m1: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                m2: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \n",
    "        m1_x = self.positional_encoding(m1)\n",
    "        m2_x = self.positional_encoding(m2)\n",
    "        print(\"passed encoding\")\n",
    "\n",
    "        m1_k = self._to_key(m1_x)\n",
    "        m1_v = self._to_query(m1_x)\n",
    "        m2_q = self._to_value(m2_x)\n",
    "        print(\"passed kqv\")\n",
    "\n",
    "        #crossing\n",
    "        cross_x, attn_weights = self.attn(m1_k, m1_v, m2_q)\n",
    "        print(\"passed attn:\", cross_x.shape)\n",
    "\n",
    "        return cross_x\n",
    "\n",
    "\n",
    "class position_wise_ffn(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Position-wise feed-forward network with a RELU activation - essentially contracts output, and squeezes it back to the same space\n",
    "\n",
    "    ARGS:\n",
    "        dim: dimension of the embeddings\n",
    "        hidden_dim: dimension of the inflated hidden layer in feed-forward network\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 hidden_dim: int, \n",
    "                 dropout: float = 0.0):\n",
    "        super(position_wise_ffn, self).__init__()\n",
    "\n",
    "        self.ffn_1 = torch.nn.Linear(dim, hidden_dim)\n",
    "        self.ffn_2 = torch.nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.ffn_1(x).relu()\n",
    "        x = self.ffn_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class cross_attn_channel(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Model for Cross Attention, architecture implementation taken from encoder layer of \"Attention is all you need\"\n",
    "    Includes multi-head attn with crossing --> add + norm --> positionwise ffn --> add + norm --> output (based on paper)\n",
    "\n",
    "    ARGS:\n",
    "        dim_m1: time series modality 1\n",
    "        dim_m2: time series modality 2\n",
    "\n",
    "    Shapes:\n",
    "        assuming seq_length is same for both\n",
    "        m1: (seq_length, N_samples, N_features)\n",
    "        m2: (seq_length, N_samples, N_features)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim_m1: int, \n",
    "                 dim_m2: int, \n",
    "                 outdim_m1: int, \n",
    "                 outdim_m2: int,\n",
    "                 heads: Optional[int], \n",
    "                 seq_len: int, \n",
    "                 dropout: float = 0.0):\n",
    "        super(cross_attn_channel, self).__init__()\n",
    "\n",
    "        self.m1_cross_m2 = cross_attn_block(dim = dim_m1, heads = heads, dropout = dropout, seq_length = seq_len)\n",
    "        self.m2_cross_m1 = cross_attn_block(dim = dim_m2, heads = heads, dropout = dropout, seq_length = seq_len)\n",
    "\n",
    "        self.norm_m1 = torch.nn.LayerNorm(dim_m1)\n",
    "        self.norm_m2 = torch.nn.LayerNorm(dim_m2)\n",
    "\n",
    "        self.m1_pffn = position_wise_ffn(dim_m1, 512)\n",
    "        self.m2_pffn = position_wise_ffn(dim_m2, 512)\n",
    "\n",
    "        self.norm_pffn_m1 = torch.nn.LayerNorm(dim_m1)\n",
    "        self.norm_pffn_m2 = torch.nn.LayerNorm(dim_m2)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                m1: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                m2: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "\n",
    "        m1_x = self.m1_cross_m2(m1, m2)\n",
    "        m2_x = self.m2_cross_m1(m2, m1)\n",
    "\n",
    "        m1_x = self.norm_m1(m1 + self.dropout(m1_x))\n",
    "        m2_x  = self.norm_m2(m2 + self.dropout(m2_x))\n",
    "\n",
    "        m1_ffn = self.m1_pffn(m1_x)\n",
    "        m2_ffn = self.m2_pffn(m2_x)\n",
    "\n",
    "        m1_x = self.norm_pffn_m1(m1_x + self.dropout(m1_ffn))\n",
    "        m2_x = self.norm_pffn_m2(m2_x + self.dropout(m2_ffn))\n",
    "\n",
    "        return m1_x, m2_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 500, 2) (510, 500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(m2_train_x.shape, m1_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_channel_ = cross_attn_channel(dim_m1 = m1_train_x.shape[-1], dim_m2 = m2_train_x.shape[-1], outdim_m1=16, outdim_m2=16, heads = 2, seq_len =  m2_train_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "m1, m2 = cross_attn_channel_(torch.Tensor(m1_train_x), torch.Tensor(m2_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([510, 500, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_train_x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iadam/miniconda3/envs/vtransformer_torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=2)\n",
    "transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "\n",
    "classifier = torch.nn.Linear(512, 2)\n",
    "out = classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 2])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing self attention channel \n",
    "\n",
    "class self_attn_block(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    self attention block \n",
    "\n",
    "    Args: \n",
    "        dim: Dimension of the embeddings for this modality \n",
    "        heads: Number of attention heads\n",
    "        dropout: Dropout rate\n",
    "        seq_length: Sequence length\n",
    "\n",
    "    Shapes: \n",
    "        x: (seq_length, N_samples, N_features)\n",
    "\n",
    "    Returns: \n",
    "        Embedding of x after self-attention with same input dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 heads: int, \n",
    "                 dropout: float, \n",
    "                 seq_length: int, \n",
    "                  add_positional: Optional[bool] = False):\n",
    "\n",
    "        super(self_attn_block, self).__init__()\n",
    "\n",
    "        self.add_positional = add_positional\n",
    "        self.positional_encoding = PositionalEncoding(dim, dropout, seq_length)\n",
    "\n",
    "        # learnable linear projections \n",
    "        self._to_key = torch.nn.Linear(dim, dim)\n",
    "        self._to_query = torch.nn.Linear(dim, dim)\n",
    "        self._to_value = torch.nn.Linear(dim, dim)\n",
    "\n",
    "        # Multi-head attention layer\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]], \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \n",
    "        # (optional) positional encoding\n",
    "        print('positional encoding...')\n",
    "        if self.add_positional:\n",
    "            x = self.positional_encoding(x)\n",
    "        print('done!')\n",
    "\n",
    "        # project input to q, k, v \n",
    "        print('k, q, v')\n",
    "        k = self._to_key(x)\n",
    "        q = self._to_query(x)\n",
    "        v = self._to_value(x)\n",
    "        print('done!')\n",
    "\n",
    "        # Self-attention: each element attends to all elements within the sequence\n",
    "        print('self attn')\n",
    "        attn_x, attn_weights = self.attn(q, k, v, attn_mask=mask)\n",
    "        print('done!')\n",
    "\n",
    "        return attn_x\n",
    "\n",
    "class self_attn_channel(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Self-Attention Channel Model, based on the architecture of \"Attention is All You Need\"\n",
    "    Includes self-attention, add + norm, position-wise FFN, add + norm\n",
    "\n",
    "    Args:\n",
    "        dim: Dimension of the embeddings\n",
    "        pffn_dim: Dimension of hidden layer in position-wise FFN\n",
    "        heads: Number of attention heads\n",
    "        seq_len: Length of sequence\n",
    "        dropout: Dropout rate\n",
    "\n",
    "    Shapes:\n",
    "        x: (seq_length, N_samples, N_features)\n",
    "\n",
    "    Returns:\n",
    "        Transformed x, same dimension as input with self-attention applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 pffn_dim: int, \n",
    "                 heads: Optional[int], \n",
    "                 seq_len: int, \n",
    "                 dropout: float = 0.0):\n",
    "        \n",
    "        super(self_attn_channel, self).__init__()\n",
    "\n",
    "        # Self-attention block\n",
    "        self.self_attn = self_attn_block(dim=dim, heads=heads, dropout=dropout, seq_length=seq_len)\n",
    "\n",
    "        # Layer normalization for self-attention output\n",
    "        self.norm_self_attn = torch.nn.LayerNorm(dim)\n",
    "\n",
    "        # Position-wise feed-forward network\n",
    "        self.pffn = position_wise_ffn(dim, pffn_dim)\n",
    "\n",
    "        # Layer normalization for FFN output\n",
    "        self.norm_pffn = torch.nn.LayerNorm(dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]], \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "\n",
    "        # self-attn and add residual connection\n",
    "        attn_x = self.self_attn(x, mask=mask)\n",
    "        x = self.norm_self_attn(x + self.dropout(attn_x))\n",
    "\n",
    "        # position-wise ffn and add residual connection\n",
    "        ffn_x = self.pffn(x)\n",
    "        x = self.norm_pffn(x + self.dropout(ffn_x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(m1_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the self attn module for m1\n",
    "m1_self_attn_channel = self_attn_channel(dim = m1_train_x.shape[-1], pffn_dim=16, heads = 2, seq_len =  m1_train_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional encoding...\n",
      "done!\n",
      "k, q, v\n",
      "done!\n",
      "self attn\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "m1_self_attn = m1_self_attn_channel(torch.Tensor(m1_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([510, 500, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_self_attn.shape # should match (seq_len, n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the MMCA model \n",
    "\n",
    "class MMCA(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Torch implentation of Multi-Modal Cross Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                #input shapes (Embedding Size, Time Length) \n",
    "                m1_shape: Optional[Tuple[int, int]] = None,\n",
    "                m2_shape: Optional[Tuple[int, int]] = None,\n",
    "                #modality 1\n",
    "                m1_self_attn_layers: Optional[int] = None,\n",
    "                m1_self_attn_heads: Optional[int] = None, \n",
    "                m1_cross_attn_layers: Optional[int] = None,\n",
    "                m1_cross_attn_heads: Optional[int] = None,\n",
    "                #modality 2\n",
    "                m2_self_attn_layers: Optional[int] = None,\n",
    "                m2_self_attn_heads: Optional[int] = None, \n",
    "                m2_cross_attn_layers: Optional[int] = None,\n",
    "                m2_cross_attn_heads: Optional[int] = None,\n",
    "                #classifier\n",
    "                classifier: Optional[Any] = None):\n",
    "\n",
    "        super(MMCA, self).__init__()\n",
    "\n",
    "        # Multi-modal self-attention layers\n",
    "        # modality 1 self-attention\n",
    "        self.m1_self_attn = nn.Sequential(*[\n",
    "            self_attn_block(m1_shape[1], m1_self_attn_heads, dropout=0.1, seq_length=m1_shape[0]) \n",
    "            for _ in range(m1_self_attn_layers)\n",
    "        ])\n",
    "        \n",
    "        # modality 1 cross-attention\n",
    "        self.m1_cross_attn = nn.Sequential(*[\n",
    "            cross_attn_block(m1_shape[1], m1_cross_attn_heads, dropout=0.1, seq_length=m1_shape[0]) \n",
    "            for _ in range(m1_cross_attn_layers)\n",
    "        ])\n",
    "        \n",
    "        # modality 2 self-attention\n",
    "        self.m2_self_attn = nn.Sequential(*[\n",
    "            self_attn_block(m2_shape[1], m2_self_attn_heads, dropout=0.1, seq_length=m2_shape[0]) \n",
    "            for _ in range(m2_self_attn_layers)\n",
    "        ])\n",
    "        \n",
    "        # modality 2 cross-attention\n",
    "        self.m2_cross_attn = nn.Sequential(*[\n",
    "            cross_attn_block(m2_shape[1], m2_cross_attn_heads, dropout=0.1, seq_length=m2_shape[0]) \n",
    "            for _ in range(m2_cross_attn_layers)\n",
    "        ])\n",
    "        \n",
    "        # classification head \n",
    "        self.classifier = classifier or nn.Linear(8, 2) # default classifier \n",
    "\n",
    "    \n",
    "    def forward(self, \n",
    "            time_series_1: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n",
    "            time_series_2: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "        #1. Run through self_attn channel\n",
    "        for layer in self.m1_self_attn:\n",
    "            m1_self_attn_x = layer(time_series_1)\n",
    "        \n",
    "        for layer in self.m2_self_attn:\n",
    "            m2_self_attn_x = layer(time_series_2)\n",
    "        \n",
    "        \n",
    "        #2. Run through cross_attn channel\n",
    "        m1_cross_attn_x = time_series_1\n",
    "        m2_cross_attn_x = time_series_2\n",
    "        for layer in self.m1_cross_attn:\n",
    "            m1_cross_attn_x = layer(m1_cross_attn_x, time_series_2)\n",
    "        \n",
    "        for layer in self.m2_cross_attn:\n",
    "            m2_cross_attn_x = layer(m2_cross_attn_x, time_series_1)\n",
    "        \n",
    "        \n",
    "        print(f\"m1_self_attn_x: {m1_self_attn_x.shape}\")\n",
    "        print(f\"m2_self_attn_x: {m2_self_attn_x.shape}\")\n",
    "        print(f\"m1_cross_attn_x: {m1_cross_attn_x.shape}\")\n",
    "        print(f\"m2_cross_attn_x: {m2_cross_attn_x.shape}\")\n",
    "        \n",
    "        #3 process (?)\n",
    "        \n",
    "        #4. Concatenate the outputs from all channels\n",
    "        print(f'concatenating channels')\n",
    "        concatenated_x = torch.cat([m1_self_attn_x, m1_cross_attn_x, m2_self_attn_x, m1_cross_attn_x], dim = -1)\n",
    "        print(f'done. concatenated_x has shape {concatenated_x.shape}')\n",
    "        \n",
    "        #5. process (?)\n",
    "        # Olivia: I am simply using mean pooling along the seq dimension to \n",
    "        # reduce to a single feature vector. \n",
    "        print(f'average pooling')\n",
    "        pooled_concat_X = concatenated_x.mean(dim=0)\n",
    "        print(f'done. pooled_concat_X has shape {pooled_concat_X.shape}')\n",
    "\n",
    "        #6. Classify the concatenated output\n",
    "        x = self.classifier(pooled_concat_X)\n",
    "\n",
    "        return x, torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmca = MMCA(\n",
    "    m1_shape=[m1_train_x.shape[0], m1_train_x.shape[-1]],  #[leq_len, n_features]\n",
    "    m2_shape=[m2_train_x.shape[0], m2_train_x.shape[-1]],\n",
    "    m1_self_attn_layers=2,\n",
    "    m1_self_attn_heads=2, \n",
    "    m1_cross_attn_layers=2,\n",
    "    m1_cross_attn_heads=2,\n",
    "    m2_self_attn_layers=2,\n",
    "    m2_self_attn_heads=2, \n",
    "    m2_cross_attn_layers=2,\n",
    "    m2_cross_attn_heads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional encoding...\n",
      "done!\n",
      "k, q, v\n",
      "done!\n",
      "self attn\n",
      "done!\n",
      "positional encoding...\n",
      "done!\n",
      "k, q, v\n",
      "done!\n",
      "self attn\n",
      "done!\n",
      "positional encoding...\n",
      "done!\n",
      "k, q, v\n",
      "done!\n",
      "self attn\n",
      "done!\n",
      "positional encoding...\n",
      "done!\n",
      "k, q, v\n",
      "done!\n",
      "self attn\n",
      "done!\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "torch.Size([510, 1, 2])\n",
      "torch.Size([510, 500, 2])\n",
      "passed encoding\n",
      "passed kqv\n",
      "passed attn: torch.Size([510, 500, 2])\n",
      "m1_self_attn_x: torch.Size([510, 500, 2])\n",
      "m2_self_attn_x: torch.Size([510, 500, 2])\n",
      "m1_cross_attn_x: torch.Size([510, 500, 2])\n",
      "m2_cross_attn_x: torch.Size([510, 500, 2])\n",
      "concatenating channels\n",
      "done. concatenated_x has shape torch.Size([510, 500, 8])\n",
      "average pooling\n",
      "done. pooled_concat_X has shape torch.Size([500, 8])\n"
     ]
    }
   ],
   "source": [
    "mmca_out = mmca(torch.Tensor(m1_train_x), torch.Tensor(m2_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mmca_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmca_out[0].shape # y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmca_out[1].shape # y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
